# main.py
# Final version - forcing a new build

import runpod
import torch
from diffusers import AnimateDiffPipeline, MotionAdapter, DDIMScheduler, ControlNetModel
from diffusers.utils import export_to_video
from transformers import CLIPImageProcessor, CLIPVisionModelWithProjection
from controlnet_aux import OpenposeDetector, MidasDetector
from huggingface_hub import hf_hub_download
from PIL import Image
import numpy as np
import base64
import io
import requests
import os
import tempfile

# ------------------------------ IP-Adapter Helper Class ----------------------------- #
class IPAdapterImageProj(torch.nn.Module):
    def __init__(self, state_dict):
        super().__init__()
        self.image_proj_model = self.init_proj(state_dict)
    def init_proj(self, state_dict):
        image_proj_model = torch.nn.Linear(state_dict["image_proj"].shape[-1], state_dict["ip_adapter"].shape[0])
        image_proj_model.load_state_dict({"weight": state_dict["image_proj"], "bias": torch.zeros(state_dict["image_proj"].shape[0])})
        return image_proj_model
    def forward(self, image_embeds):
        return self.image_proj_model(image_embeds)

# --------------------------------- Globals for Lazy-Loading --------------------------------- #
pipe = None
image_encoder = None
image_proj_model = None
image_processor = None
openpose_detector = None
midas_detector = None

# ------------------------------ File Upload Utility ----------------------------- #
def upload_to_catbox(filepath):
    try:
        with open(filepath, 'rb') as f:
            files = {'fileToUpload': (os.path.basename(filepath), f)}
            data = {'reqtype': 'fileupload'}
            response = requests.post('https://litterbox.catbox.moe/resources/internals/api.php', files=files, data=data)
            response.raise_for_status()
            return response.text
    except Exception as e:
        return f"Error uploading: {str(e)}"

# --------------------------------- Job Handler ---------------------------------- #
def generate_video(job):
    global pipe, image_encoder, image_proj_model, image_processor, openpose_detector, midas_detector

    if pipe is None:
        print("‚è≥ Loading models for the first time...")
        
        # ‚úÖ Get the Hugging Face token from the environment variables
        HUGGING_FACE_TOKEN = os.getenv("HUGGING_FACE_HUB_TOKEN")
        if HUGGING_FACE_TOKEN:
            print("üîê Using HF token:", HUGGING_FACE_TOKEN[:8] + "...")
        else:
            print("‚ö†Ô∏è HF token not found. Downloads may fail for gated models.")
            
        base_model_id = "SG161222/Realistic_Vision_V5.1_noVAE"
        motion_module_id = "guoyww/animatediff-motion-module-v3"
        ip_adapter_repo_id = "h94/IP-Adapter"
        
        openpose_controlnet = ControlNetModel.from_pretrained("lllyasviel/control_v11p_sd15_openpose", torch_dtype=torch.float16, token=HUGGING_FACE_TOKEN)
        depth_controlnet = ControlNetModel.from_pretrained("lllyasviel/control_v11f1p_sd15_depth", torch_dtype=torch.float16, token=HUGGING_FACE_TOKEN)
        openpose_detector = OpenposeDetector.from_pretrained("lllyasviel/ControlNet", token=HUGGING_FACE_TOKEN)
        midas_detector = MidasDetector.from_pretrained("lllyasviel/ControlNet", token=HUGGING_FACE_TOKEN)
        
        pipe = AnimateDiffPipeline.from_pretrained(
            base_model_id,
            controlnet=[openpose_controlnet, depth_controlnet],
            torch_dtype=torch.float16,
            token=HUGGING_FACE_TOKEN
        )
        pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)
        pipe.load_motion_module(motion_module_id, unet_additional_kwargs={"use_inflated_groupnorm": True}, token=HUGGING_FACE_TOKEN)
        pipe.enable_model_cpu_offload()

        image_encoder = CLIPVisionModelWithProjection.from_pretrained(
            ip_adapter_repo_id, subfolder="models/image_encoder", torch_dtype=torch.float16, token=HUGGING_FACE_TOKEN
        ).to("cuda")
        image_processor = CLIPImageProcessor.from_pretrained(
            ip_adapter_repo_id, subfolder="models/image_encoder", token=HUGGING_FACE_TOKEN
        )
        ip_adapter_path = hf_hub_download(repo_id=ip_adapter_repo_id, filename="ip-adapter_sd15.bin", token=HUGGING_FACE_TOKEN)
        ip_adapter_weights = torch.load(ip_adapter_path, map_location="cpu")
        image_proj_model = IPAdapterImageProj(ip_adapter_weights).to("cuda")
        
        print("‚úÖ Models loaded successfully.")

    job_input = job.get('input', {})
    base64_image = job_input.get('init_image')
    prompt = job_input.get('prompt', 'a couple kissing, beautiful, cinematic')
    
    num_frames = int(job_input.get('num_frames', 16))
    fps = int(job_input.get('fps', 8))
    ip_adapter_scale = min(max(float(job_input.get('ip_adapter_scale', 0.7)), 0.0), 1.5)
    openpose_scale = min(max(float(job_input.get('openpose_scale', 1.0)), 0.0), 1.5)
    depth_scale = min(max(float(job_input.get('depth_scale', 0.5)), 0.0), 1.5)

    if not base64_image:
        return {"error": "Missing 'init_image' base64 input."}

    try:
        init_image = Image.open(io.BytesIO(base64.b64decode(base64_image))).convert("RGB")
    except Exception as e:
        return {"error": f"Image decode error: {str(e)}"}

    print("üîç Preprocessing image...")
    processed_image = image_processor(images=init_image, return_tensors="pt").pixel_values.to("cuda", dtype=torch.float16)
    clip_features = image_encoder(processed_image).image_embeds
    image_embeds = image_proj_model(clip_features)
    openpose_image = openpose_detector(init_image)
    depth_image = midas_detector(init_image)
    control_images = [openpose_image, depth_image]
    
    cross_attention_kwargs = {"ip_adapter_image_embeds": image_embeds, "scale": ip_adapter_scale}

    print("‚ú® Inference started...")
    output = pipe(
        prompt=prompt,
        negative_prompt="ugly, distorted, low quality, cropped, blurry",
        num_frames=num_frames,
        guidance_scale=7.5,
        num_inference_steps=20,
        image=control_images,
        controlnet_conditioning_scale=[openpose_scale, depth_scale],
        cross_attention_kwargs=cross_attention_kwargs
    )
    frames = output.frames[0]
    
    print("üìº Exporting video...")
    with tempfile.TemporaryDirectory() as tmpdir:
        video_path = os.path.join(tmpdir, "kissify.mp4")
        export_to_video(frames, video_path, fps=fps)
        
        print("üöÄ Uploading video...")
        url = upload_to_catbox(video_path)
        if "Error" in url:
            return {"error": url}
        return {"output": {"video_url": url}}

runpod.serverless.start({"handler": generate_video})
